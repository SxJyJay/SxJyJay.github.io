<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yang Jiao</title>

    <meta name="author" content="Yang Jiao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yang Jiao (焦洋)
                </p>
                <p>I'm currently a fourth-year Ph.D student at <a href="https://fvl.fudan.edu.cn/">FVL Lab</a> of Fudan University, advised by Prof. <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ&hl=zh-CN">Yu-Gang Jiang</a> and <a href="https://jingjing1.github.io/">Jingjing Chen</a>. Before that, I received my Bachelor's degree in <a href="https://www.yingcai.uestc.edu.cn/">Yingcai Honors College</a> of UESTC in 2021. I also work closely with <a href="https://scholar.google.com/citations?hl=zh-CN&user=4sKGNB0AAAAJ&view_op=list_works&sortby=pubdate">Dr. Zequn Jie</a>, <a href="https://forestlinma.com/">Dr. Lin Ma</a>, and <a href="https://scholar.google.com/citations?hl=zh-CN&user=WL5mbfEAAAAJ&view_op=list_works&sortby=pubdate">Dr. Shaoxiang Chen</a> during my internship at Meituan.
                </p>
                <p>
                  My research interests include Multimodal Large Language Model, Autonomous Driving and 3D Vision.
                </p>
                <p style="text-align:center">
                  <a href="yjiao23@m.fudan.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/YangJiao-CV-CN.pdf">CV(CN)</a>/<a href="data/YangJiao-CV-EN.pdf">(EN)</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=5gA7Wv0AAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/SxJyJay/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/YangJiao.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0%;" alt="profile photo" src="images/YangJiao.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Apr. 2025:</span> Our <a href="https://arxiv.org/abs/2504.04423">UniToken</a>, a Multimodal Large Language Model supporting unified visual understanding and image generation with advanced performances, is accepted to CVPRW 2025!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Oct. 2024:</span> Our <a href="https://openreview.net/pdf?id=v5Un2QqnRf">Lumen</a>, a Multimodal Large Language Model with versatile vision-centric capabilities, is accepted to NeurIPS 2024!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Apr. 2024:</span> I am supported by the Fundamental Research Project for Young Professionals from NSFC (首批国自然博士基金)!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Sept. 2023:</span> One about Generalized Food Recognition is accepted to TMM 2024!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Sept. 2023:</span> Two papers about Autonomous Driving are accepted to AAAI 2024!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Jul. 2023:</span> One paper about Visual Grounding is accepted to ACM MM 2023!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Mar. 2023:</span> Our <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">MSMDFusion</a>, ranking 1st and 2nd at <a gref="https://www.nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Any">NuScenes detection</a> and <a href="https://www.nuscenes.org/tracking?externalData=all&mapData=all&modalities=Any">tracking</a> benchmarks among all single-model submission (at the time of submission), is accepted to CVPR 2023!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Jul. 2022:</span> One paper about 3D Dense Captioning is accepted to ECCV 2022!
		</p>
		<p>
		  ▪️ <span class="highlight" style="width: 80px;">Jul. 2021:</span> One paper about Referring Image Segmentation is accepted to ACM MM 2021!
		</p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2> 
	      </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/UniToken.png" alt="unitoken" width="160" height="80" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://arxiv.org/abs/2504.04423">
          <span class="papertitle">UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
	Haibo Qiu,
	Zequn Jie,
	Shaoxiang Chen,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>CVPRW</em>, 2025
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/SxJyJay/UniToken">code</a>/<a href="https://arxiv.org/abs/2504.04423">paper</a>
        <p></p>
        <p>
          UniToken is an auto-regressive generation model that combines discrete and continuous representations to process visual inputs, making it easy to integrate both visual understanding and image generation tasks seamlessly. 
        </p>
      </td>
    </tr>

		  
		  
    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/Lumen_teasure_1.png" alt="lumen" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://openreview.net/pdf?id=v5Un2QqnRf">
          <span class="papertitle">Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
        Shaoxiang Chen,
	Zequn Jie,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/SxJyJay/Lumen">code</a>/<a href="https://openreview.net/pdf?id=v5Un2QqnRf">paper</a>
        <p></p>
        <p>
          Enhancing the versatile vision-centric capabilities of existing MLLMs, such as object detection, instance segmentation, and pose estimation, without compromising their general-purpose question-answering abilities.
        </p>
      </td>
    </tr>

    <style>
      .zoomable {
        cursor: pointer;
        transition: transform 0.3s ease;
      }
      .zoomable.zoomed {
        transform: scale(2); /* 放大比例 */
        z-index: 10;
        position: relative;
      }
    </style>

    <script>
      function zoomImage(img) {
        img.classList.toggle("zoomed"); // 点击切换 zoomed 类
      }
    </script>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/IABEV_teasure.png" alt="iabev" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28037">
          <span class="papertitle">Instance-Aware Multi-Camera 3D Object Detection with Structural Priors Mining and Self-Boosting Learning</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
        Zequn Jie,
	Shaoxiang Chen,
	Lechao Cheng,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>AAAI</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28037">paper</a>
        <p></p>
        <p>
          Mining image-plane instance awareness from category-centric and instance-centric perspectives for enhanced depth estimation in the multi-view camera-based object detection.  
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/NuscenesQA_teasure.png" alt="nuscenesqa" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28253">
          <span class="papertitle">NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        Tianwen Qian,
	Jingjing Chen,
	Linhai Zhuo,
	<strong>Yang Jiao</strong>,
	Yu-Gang Jiang
        <br>
        <em>AAAI</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/qiantianwen/NuScenes-QA">code</a>/<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28253">paper</a>
        <p></p>
        <p>
          Introducing a novel visual question answering (VQA) task in the context of autonomous driving, aiming to answer natural language questions based on street-view clues.  
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/TMM_teasure.png" alt="tmm" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://ieeexplore.ieee.org/document/10453509">
          <span class="papertitle">From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        Guoshan Liu,
	<strong>Yang Jiao</strong>,
	Jingjing Chen,
	Bin Zhu,
	Yu-Gang Jiang
        <br>
        <em>TMM</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/SLKAlgs/DailyFood-172">code</a>/<a href="https://ieeexplore.ieee.org/document/10453509">paper</a>
        <p></p>
        <p>
          Presenting DailyFood-172 and DailyFood-16, both contain food images from everyday meals. And introducing a simple yet effective baseline method for mitigating the domain gap between well-curated and daily-life food images. 
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/SOT_teasure.png" alt="sot" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://dl.acm.org/doi/10.1145/3581783.3611721">
          <span class="papertitle">Suspected Objects Matter: Rethinking Model's Prediction for One-stage Visual Grounding</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
	<strong>Yang Jiao</strong>,
	Zequn Jie,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>ACM MM</em>, 2023
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://dl.acm.org/doi/10.1145/3581783.3611721">paper</a>
        <p></p>
        <p>
          Explicitly mining inter-object relationships in the one-stage visual grounding paradigm. The proposed technique can be seamlessly integrated into both CNN and Transformer-based architectures.   
        </p>
      </td>
    </tr>

		  
    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/MSMD_teasure.png" alt="msmdfusion" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">
          <span class="papertitle">MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
        Zequn Jie,
	Shaoxiang Chen,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>CVPR</em>, 2023
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/SxJyJay/MSMDFusion">code</a>/<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">paper</a>
        <p></p>
        <p>
          Fusing LiDAR and camera signals in multi-scale voxel fields for enhanced BEV representation. Ranking 1st and 2nd on NuScenes detection and tracking benchmarks among all single models by the time of submission.  
        </p>
      </td>
    </tr>



    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/MORE_teasure.png" alt="more" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://arxiv.org/abs/2203.05203">
          <span class="papertitle">MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
	Shaoxiang Chen,
	Zequn Jie,
	Jingjing Chen,
	Lin Ma,
	Yu-Gang Jiang
        <br>
        <em>ECCV</em>, 2022
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/SxJyJay/MORE">code</a>/<a href="https://arxiv.org/abs/2203.05203">paper</a>
        <p></p>
        <p>
            Progressively mining inter-object spatial relationships, advancing from single-order to multi-order interactions, to generate more descriptive and comprehensive captions for 3D scenes.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/TVNet_teasure.png" alt="tvnet" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475222">
          <span class="papertitle">Two-stage Visual Cues Enhancement Network for Referring Image Segmentation</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
        <strong>Yang Jiao</strong>,
	Zequn Jie,
	Weixin Luo,
	Jingjing Chen,
	Yu-Gang Jiang,
	Xiaolin Wei,
	Lin Ma
        <br>
        <em>ACM MM</em>, 2021
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475222">paper</a>
        <p></p>
        <p>
            Enhancing the visual cues of referred objects in the image via a two-stage pipeline in the referring image segmentation.
        </p>
      </td>
    </tr>
		  
		  
    

          </tbody></table>

          
					

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Preprints</h2> 
	      </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


		  
    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/EventHallusion_teasure.png" alt="eventhallusion" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://arxiv.org/abs/2409.16597">
          <span class="papertitle">EventHallusion: Diagnosing Event Hallucinations in Video LLMs</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
	Jiacheng Zhang*,
        <strong>Yang Jiao*</strong>,
	Shaoxiang Chen,
	Na Zhao,
	Jingjing Chen (* denotes equal contribution)
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://github.com/Stevetich/EventHallusion">code</a>/<a href="https://arxiv.org/abs/2409.16597">paper</a>
        <p></p>
        <p>
          Curating a novel benchmark aiming to diagnose the susceptibility of existing VideoLLMs toward language priors and spurious vision-language correspondence. And a simple-yet-effective Temporal Contrastive Decoding (TCD) method is proposed to alleviate such event-oriented hallucinations.
        </p>
      </td>
    </tr>


     <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <!-- 只显示图片 -->
          <img src="images/LLAVAAD_teasure.png" alt="llavaad" width="160" height="160" class="zoomable" onclick="zoomImage(this)">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- 修改文章标题和链接 -->
        <a href="https://arxiv.org/abs/2404.12966">
          <span class="papertitle">Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning</span>
        </a>
        <br>
    
        <!-- 修改作者信息 -->
	Yian Li*,
	Wentao Tian*,
        <strong>Yang Jiao</strong>,
	Jingjing Chen,
	Na Zhao,
	Yu-Gang Jiang (* denotes equal contribution)
        <br>
        <em>Arxiv</em>, 2024
        <br>
        <!-- 修改文章页面链接 -->
        <a href="https://arxiv.org/abs/2409.16597">paper</a>
        <p></p>
        <p>
          Curating a novel benchmark aiming to assess human-like composite reasoning capabilities of existing MLLMs. And a simple-yet-effective method, Active Deduction (AD), is proposed to encourage the model to actively perform composite deduction before reaching a final decision.
        </p>
      </td>
    </tr>



	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Miscellanea</h2> 
	      </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Awards</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                2023 First-class Doctoral Scholarship
                <br>
                2022 DongShi Enterprise Scholarship
                <br>
                2021 National Scholarship
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                Reviewer for CVPR 2025, ICML 2025 
                <br>
                Reviewer for ICLR 2024, ICML 2024, NeurIPS 2024, CVPR 2024, AAAI 2024, IJICAI 2024
                <br>
                Reviewer for IJICAI 2023
              </td>
            </tr>
		  
						
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This page is inspired by and adapted with gratitude from this <a href="https://github.com/jonbarron/jonbarron_website">website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
